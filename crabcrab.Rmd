---
title: "Crab Research Proj"
output:
  html_document:
    df_print: paged
---

### Team:
- Alina Muliak
- Olexiy Hoiev
- Oleksandra Stasiuk


```{r}
library(ggplot2)
```

```{r}
data <- read.csv("./CrabAgePrediction.csv", TRUE)

data <- data[,-6:-8]
data <- data[order(data$Age),]
data$Length[1:100]
plot(data, col="purple")
plot(data$Height, col="purple")
```

```{r}
ggplot(data) +
    geom_density(aes(x = Length, fill = Sex), alpha = 0.5) +
    labs(y = "Density", x = "Length", fill = "Nitrogen\nConcentration") +
    scale_fill_manual(labels = c("M", "F", "I"),
        values = c("#DB24BC", "#BCDB24", "#24BCDB")) +
    theme_minimal()
ggplot(data) +
    geom_density(aes(x = Age, fill = Sex), alpha = 0.6) +
    labs(y = "Density", x = "Age", fill = "Nitrogen\nConcentration") +
    scale_fill_manual(labels = c("M", "F", "I"),
        values = c("#DB24BC", "#BCDB24", "#24BCDB")) +
    theme_minimal()
head(data)
```

```{r}
mydata <- read.csv("./CrabAgePrediction.csv", TRUE)
summary(mydata)
```
The good measure to test how good is our model is the **coefficient of determination** $R^2$. This measure is defined by the proportion of the total variability explained by the regression model.
$$R^2 = \frac{\text{Explained variation of the model}}{\text{Total variation of the model}}$$
We know, that for models that fit the data well, $R^2$ is near $1$ and conversely, models that poorly fit the data have $R^2$ near $0$.
```{r}
height <- lm(Age ~ Height, data = mydata)
summary(height)
```
Here we can see that this model has an $R^2$ of $0.3$; this means that the model explains only $30\%$ of the data variability, which is not the desired and satisfactory result for us. So, probably `Height` does not tell much about crab's age.

```{r}
weight.lenght <- aov(Age ~ Weight + Length, data = mydata)

summary(weight.lenght)
```
Adding **length**  to the model seems to have made the model better: it reduced the residual variance (the residual sum of squares went from 28655  to 27762), and both weight and length are statistically significant (p-values < 0.001).
\\Let's add some more:
```{r}
all.parameters <- aov(Age ~ Weight + Diameter + Length + Sex + Height, data = mydata)

summary(all.parameters)
```
Less squared sum of residual: 25487

```{r}
interaction <- aov(Age ~ Length*Diameter, data = mydata)

summary(interaction)
```
The 'Length:Diameter' variable has a low sum-of-squares value and still quite low p-value, which means there is variation that can be explained by the interaction between Diameter and Length.
-> можемо побачити, що diameter і length дуже сильно залежні між собою, тому можемо спробувати вибрати щось одне з них, і бачимо, що якщо забрати Length, то модель стане не набагато гіршою, але кількість залежностей зменшиться.


```{r}
all.without.length <- aov(Age ~ Weight + Diameter + Sex + Height, data = mydata)

summary(all.without.length)
```

```{r}
library(AICcmodavg)

model.set <- list(weight.lenght, all.parameters, interaction, all.without.length)
model.names <- c("weight.lenght", "all.parameters", "interaction", "all.without.length")

aictab(model.set, modnames = model.names)
```
